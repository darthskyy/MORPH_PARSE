{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b50e4a7",
   "metadata": {},
   "source": [
    "# Fine Tuning XLM RoBERTa models for Morphological Parsing\n",
    "This project is one half of the Honours Project by [Cael Marquard](https://github.com/Restioson \"Cael Marquard GitHub\") and [Simbarashe Mawere](https://github.com/darthskyy \"Simbarashe Mawere GitHub\") to use deep learning methods for the task of morphological parsing. This half consists of fine-tuning pretrained models for the task. The selected models are from the Hugging Face transformers library and are as follows:\n",
    "1. [XLM-RoBERTa (large)](https://huggingface.co/FacebookAI/xlm-roberta-large \"XLM-RoBERTa Large by FacebookAI\")\n",
    "2. [AfroXLMR (76L)](https://huggingface.co/Davlan/afro-xlmr-large-76L \"Afro XLMR Large by David Adelani\")\n",
    "3. [NguniXLMR](https://huggingface.co/francois-meyer/nguni-xlmr-large \"Nguni XLMR Large by Francois Meyer\")\n",
    "\n",
    "To fine-tune XLM RoBERTa models for morphological parsing, you will need the following packages:\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- argparse\n",
    "- warnings\n",
    "- time\n",
    "- logging\n",
    "- sys\n",
    "- torch\n",
    "- datasets\n",
    "- os\n",
    "- csv\n",
    "- transformers\n",
    "- datasets\n",
    "- seqeval\n",
    "\n",
    "THIS WAS FROM THE FIRST RUNS. NOW OBSOLETE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a665e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the packages required for the script\n",
    "%pip install pandas numpy argparse torch transformers transformers[torch] seqeval datasets csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29233abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os, csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import warnings\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# * load the arguments from the command line\n",
    "parser = argparse.ArgumentParser(description=\"Parsing inputs for training the model\")\n",
    "# model data, loading and saving arguments\n",
    "parser.add_argument(\"--server\", type=str, default=\"local\", help=\"The server to run the script on. If this is set to anything other than local, data directory will be inferred.\", choices=[\"local\", \"uct\", \"nicis\"])\n",
    "parser.add_argument(\"--data\", type=str, default=\"../data\", help=\"The directory where the data is stored.\")\n",
    "parser.add_argument(\"--lang\", type=str, default=\"NR\", help=\"The language to train the model on.\", choices=[\"NR\",\"SS\",\"XH\",\"ZU\"])\n",
    "parser.add_argument(\"--checkpoint\", type=str, default=\"xlm-roberta-base\", help=\"The pretrained checkpoint to use for the model. Must be a model that supports token classification.\")\n",
    "parser.add_argument(\"--resume_from_checkpoint\", action=\"store_true\", help=\"Whether to resume training from a checkpoint.\")\n",
    "parser.add_argument(\"--output\", type=str, default=\"xlmr\", help=\"The output directory for the model in the models directory.\")\n",
    "\n",
    "# training arguments\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"The seed to use for reproducibility.\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=1, help=\"The number of epochs to train the model for.\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=16, help=\"The batch size to use for training and evaluation.\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=2e-5, help=\"The learning rate to use for training.\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0.01, help=\"The weight decay to use for training.\")\n",
    "parser.add_argument(\"--evaluation_strategy\", type=str, default=\"steps\", help=\"The evaluation strategy to use for training.\", choices=[\"epoch\", \"steps\"])\n",
    "parser.add_argument(\"--validation_split\", type=float, default=0.1, help=\"The fraction of the training data to use for validation.\")\n",
    "parser.add_argument(\"--save_steps\", type=int, default=500, help=\"The number of steps to save the model after.\")\n",
    "parser.add_argument(\"--save_total_limit\", type=int, default=2, help=\"The total number of models to save.\")\n",
    "parser.add_argument(\"--metric\", type=str, default=\"all\", help=\"The metric to use for evaluation.\", choices=[\"all\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "# training flags\n",
    "parser.add_argument(\"--load_best_model_at_end\", action=\"store_true\", help=\"Whether to load the best model at the end of training.\")\n",
    "parser.add_argument(\"--metric_for_best_model\", type=str, default=\"loss\", help=\"The metric to use for the best model.\")\n",
    "parser.add_argument(\"--greater_is_better\", type=bool, default=False, help=\"Whether a greater value of the metric is better.\")\n",
    "parser.add_argument(\"--warning\", type=bool, default=False, help=\"Whether to show warnings or not.\")\n",
    "\n",
    "# debugging and logging\n",
    "parser.add_argument(\"--f\", type=str, default=\"morpheme\", help=\"The field to use for the morphemes.\")\n",
    "parser.add_argument(\"--debug\", type=bool, default=True, help=\"Whether to run the script in debug mode.\")\n",
    "parser.add_argument(\"--log\", type=str, default=\"train.log\", help=\"The log file to write to.\")\n",
    "parser.add_argument(\"--verbose\", type=bool, default=True, help=\"Whether to show verbose output or not.\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# * checking correctness of the arguments in relation to each other\n",
    "if args.evaluation_strategy == \"steps\":\n",
    "    assert args.save_steps, \"The save steps must be specified when using steps evaluation strategy.\"\n",
    "\n",
    "if args.load_best_model_at_end:\n",
    "    assert args.evaluation_strategy == \"steps\", \"The evaluation strategy must be steps when loading the best model at the end.\"\n",
    "\n",
    "# * show warnings if the warning flag is set\n",
    "if not args.warning:\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# * setting up the logging\n",
    "logger = logging.getLogger(f\"train_{args.lang}_{args.checkpoint}\")\n",
    "logger.setLevel(\"DEBUG\")\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)-8s - %(message)s\")  # Modified line\n",
    "\n",
    "# setting up the file handler\n",
    "fh = logging.FileHandler(args.log)\n",
    "fh.setLevel(\"DEBUG\")\n",
    "\n",
    "# setting up the console handler\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "ch.setLevel(\"DEBUG\")\n",
    "\n",
    "# setting up the formatter\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "# adding the handlers to the logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "logger.debug(\"Logging setup complete\")\n",
    "# format the arguments better\n",
    "def format_args(args):\n",
    "    out = \"\"\n",
    "    for arg in vars(args):\n",
    "        # put quotes around strings\n",
    "        if isinstance(getattr(args, arg), str):\n",
    "            out += f\"\\t{arg:25}: '{getattr(args, arg)}'\\n\"\n",
    "        else:\n",
    "            out += f\"\\t{arg:25}: {getattr(args, arg)}\\n\"\n",
    "    return out\n",
    "\n",
    "logger.info(f\"\\nSetup Arguments Parsed\\n{format_args(args)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5189f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * checking for gpu availability\n",
    "USING_GPU = torch.cuda.is_available()\n",
    "logger.info(f\"Using GPU: {USING_GPU}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499aefb3",
   "metadata": {},
   "source": [
    "## Data Pre-processing for the Model\n",
    "We read in the files from the data directory. The data directory must be in the form:\n",
    "<br>\n",
    "data/<br>\n",
    "├── TRAIN/ <br>\n",
    "│ ├── NR_TRAIN.tsv<br>\n",
    "│ ├── SS_TRAIN.tsv<br>\n",
    "│ ├── XH_TRAIN.tsv<br>\n",
    "│ └── ZU_TRAIN.tsv<br>\n",
    "├── TEST/<br>\n",
    "│ ├── NR_TEST.tsv<br>\n",
    "│ ├── SS_TEST.tsv<br>\n",
    "│ ├── XH_TEST.tsv<br>\n",
    "│ └── ZU_TEST.tsv<br>\n",
    "└── README.txt\n",
    "\n",
    "### Reading in file\n",
    "The files are TSVs with the columns: [\"word\", \"parsed\", \"morpheme\", \"tag\"] where each line is a new word. They are read in with pandas and a validation set is extracted from the train set by samplings a percentage of the words using a specified seed.\n",
    "\n",
    "### Extraction and Indexing Words\n",
    "For the tokenization of the tags, we first index them so that they are better tokenized as integers by the models. All the possible tags are read into a set and each tag is given an integer index for the id2label and label2id (mappings and mappings_r) references. Only the \"morpheme\" and \"tag\" columns are used. The tag column used for the index mapping. Since both columns consist of '_' separated strings we split them into individual morphemes and tags, e.g., \"a_nga_s_e\" is split into [a, nga, s, e].\n",
    "\n",
    "### Loading and Tokenization\n",
    "The morphemes column is tokenized using the XLMRobertaTokenizerFast class from Hugging Face and the tags are aligned to account for subword tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab32dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset for the specified language\n",
    "column_names = [\"word\", \"parsed\", \"morpheme\", \"tag\"]\n",
    "\n",
    "# TODO add the download data flag\n",
    "try:\n",
    "    lang_set = {\n",
    "        \"TRAIN\": pd.read_csv(f\"{args.data}/TRAIN/{args.lang}_TRAIN.tsv\", delimiter=\"\\t\", quoting=csv.QUOTE_NONE, names=column_names)\n",
    "        ,\n",
    "        \"TEST\": pd.read_csv(f\"{args.data}/TEST/{args.lang}_TEST.tsv\", delimiter=\"\\t\", quoting=csv.QUOTE_NONE, names=column_names,)\n",
    "        ,\n",
    "    }\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"File not found: {e}\")\n",
    "    logger.info(f\"Files can be found for download at: https://repo.sadilar.org/handle/20.500.12185/546.\")\n",
    "    logger.info(\"Please download the files and place them in the data directory. (or use the --download_data flag to download the data)\")\n",
    "    sys.exit(1)\n",
    "# split the training data into training and validation sets\n",
    "\n",
    "lang_set[\"VAL\"] = lang_set[\"TRAIN\"].sample(frac=args.validation_split, random_state=args.seed)\n",
    "lang_set[\"TRAIN\"] = lang_set[\"TRAIN\"].drop(lang_set[\"VAL\"].index)\n",
    "\n",
    "\n",
    "logger.debug(\"loaded the datasets\")\n",
    "logger.info(f\"Training set: {len(lang_set['TRAIN'])}\")\n",
    "logger.info(f\"Validation set: {len(lang_set['VAL'])}\")\n",
    "logger.info(f\"Test set: {len(lang_set['TEST'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14026cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * map the rags to corresponding integers\n",
    "mappings = {}\n",
    "mappings_r = {}\n",
    "count = 0\n",
    "def extract_tag(seq: str) -> str:\n",
    "    global mappings, count\n",
    "    seq = seq.split(\"_\")\n",
    "    for i, tag in enumerate(seq):\n",
    "        if tag not in mappings.keys():\n",
    "            mappings[tag] = count\n",
    "            mappings_r[count] = tag\n",
    "            count+=1\n",
    "        seq[i] = mappings[tag]\n",
    "    return seq\n",
    "\n",
    "for item in [\"TEST\", \"TRAIN\", \"VAL\"]:\n",
    "    df = lang_set[item]\n",
    "    df['morpheme'] = df['morpheme'].apply(lambda x: x.split(\"_\"))\n",
    "    df['tag'] = df['tag'].apply(lambda x: extract_tag(x))\n",
    "\n",
    "logger.debug(\"mapped the input\")\n",
    "logger.info(f\"No. of tags: {len(mappings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * create the dataset\n",
    "dataset = {\n",
    "    \"train\": Dataset.from_pandas(lang_set[\"TRAIN\"]),\n",
    "    \"test\": Dataset.from_pandas(lang_set[\"TEST\"]),\n",
    "    \"validation\": Dataset.from_pandas(lang_set[\"VAL\"])\n",
    "}\n",
    "\n",
    "lang_set = DatasetDict(dataset)\n",
    "logger.debug(\"datasets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc6175",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# * loading the tokenizer and model\n",
    "from transformers import XLMRobertaTokenizerFast, AutoModelForTokenClassification\n",
    "checkpoint = args.checkpoint\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(checkpoint, cache_dir=\".cache\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=len(mappings), cache_dir=\".cache\")\n",
    "\n",
    "logger.debug(\"loaded the model and tokenizer\")\n",
    "logger.info(f\"Model: {checkpoint}\")\n",
    "\n",
    "if USING_GPU:\n",
    "    model.to(\"cuda\")\n",
    "    logger.info(\"Model on GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0463f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * tokenizing the input\n",
    "def tokenize_and_align(example, label_all_tokens=True):\n",
    "    \"\"\"\n",
    "    Tokenizes the input example and aligns the labels with the tokenized input.\n",
    "\n",
    "    Args:\n",
    "        example (dict): The input example containing the \"morpheme\" and \"tag\" fields.\n",
    "        label_all_tokens (bool, optional): Whether to include labels for all tokens. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        dict: The tokenized input with aligned labels.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenized_input = tokenizer(example[\"morpheme\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(example[\"tag\"]):\n",
    "        word_ids = tokenized_input.word_ids(batch_index=i)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_input[\"labels\"] = labels\n",
    "    return tokenized_input\n",
    "\n",
    "tokenized_dataset = lang_set.map(tokenize_and_align, batched=True)\n",
    "\n",
    "logger.debug(\"tokenized the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86957ee",
   "metadata": {},
   "source": [
    "## Training\n",
    "We use the stock Hugging Face model Trainer class and load the parsed command-line arguments using the TrainingArguments class.\n",
    "\n",
    "### Metrics\n",
    "Since the key metrics for this project are Precision, Recall and F1, we just make use of the scikit-learn metrics for the classification report which gives the averages based on all of the classes. We use both micro and macro averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab62d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# * loading the arguments and training the model\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=args.output,\n",
    "    evaluation_strategy=args.evaluation_strategy,\n",
    "    learning_rate=args.learning_rate,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=args.epochs,\n",
    "    weight_decay=args.weight_decay,\n",
    "    logging_dir=args.output+\"/logs\",\n",
    "    save_steps=args.save_steps,\n",
    "    save_total_limit=args.save_total_limit,\n",
    "    disable_tqdm=not args.debug,\n",
    "    load_best_model_at_end=args.load_best_model_at_end,\n",
    "    metric_for_best_model=args.metric_for_best_model,\n",
    "    greater_is_better=args.greater_is_better,\n",
    "    resume_from_checkpoint=args.resume_from_checkpoint,\n",
    "    seed=args.seed,\n",
    ")\n",
    "\n",
    "# print all the entered args\n",
    "print(f\"output directory: {train_args.output_dir}\")\n",
    "print(f\"logging directory: {train_args.logging_dir}\")\n",
    "print(f\"evaluation strategy: {train_args.evaluation_strategy}\")\n",
    "print(f\"learning rate: {train_args.learning_rate}\")\n",
    "print(f\"epochs: {train_args.num_train_epochs}\")\n",
    "print(f\"weight decay: {train_args.weight_decay}\")\n",
    "print(f\"save steps: {train_args.save_steps}\")\n",
    "print(f\"save total limit: {train_args.save_total_limit}\")\n",
    "print(f\"per device train batch size: {train_args.per_device_train_batch_size}\")\n",
    "print(f\"per device eval batch size: {train_args.per_device_eval_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab1d49",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# * defining the compute metrics function\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, labels = eval_preds\n",
    "    pred_logits = np.argmax(pred_logits, axis=2)\n",
    "\n",
    "    predictions = [\n",
    "        [\"_-\"+mappings_r[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] for prediction, label in zip(pred_logits, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [\"_-\"+mappings_r[l] for (eval_preds, l) in zip(prediction, label) if l != -100] for prediction, label in zip(pred_logits, labels)\n",
    "    ]\n",
    "\n",
    "    results = classification_report(true_labels, predictions, output_dict=True)\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"micro avg\"][\"precision\"],\n",
    "        \"recall\": results[\"micro avg\"][\"recall\"],\n",
    "        \"f1\": results[\"micro avg\"][\"f1-score\"] ,\n",
    "        \"macro-f1\": results[\"macro avg\"][\"f1-score\"],\n",
    "        \"macro-precision\": results[\"macro avg\"][\"precision\"],\n",
    "        \"macro-recall\": results[\"macro avg\"][\"recall\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20be4f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * adding the trainer\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "logger.debug(\"added the trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e73c061",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# * saving the model, tokenizer and mappings\n",
    "import json\n",
    "model.save_pretrained(args.output)\n",
    "tokenizer.save_pretrained(args.output)\n",
    "\n",
    "config = json.load(open(f\"{args.output}/config.json\"))\n",
    "config[\"id2label\"] = mappings_r\n",
    "config[\"label2id\"] = mappings\n",
    "\n",
    "model.config.id2label = mappings_r\n",
    "model.config.label2id = mappings\n",
    "\n",
    "json.dump(config, open(f\"{args.output}/config.json\", \"w\"))\n",
    "\n",
    "logger.debug(\"saved the model, tokenizer and mappings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * training the model\n",
    "# check if the model is to be resumed from a checkpoint\n",
    "if args.resume_from_checkpoint:\n",
    "    logger.debug(\"checking for checkpoint\")\n",
    "    dirs = os.listdir(f\"{args.output}\")\n",
    "    if any([x for x in dirs if \"checkpoint\" in x]):\n",
    "        resume_points = [x for x in dirs if \"checkpoint\" in x]\n",
    "        steps = [int(x.split(\"-\")[1]) for x in resume_points]\n",
    "        # get the checkpoint with the highest number of steps\n",
    "        resume_point = resume_points[np.argmax(steps)]\n",
    "        args.resume_from_checkpoint = f\"{args.output}/{resume_point}\"\n",
    "        logger.info(f\"Resuming from checkpoint: {args.resume_from_checkpoint}\")\n",
    "        logger.info(f\"Resuming from step: {max(steps)}\")\n",
    "    else:\n",
    "        args.resume_from_checkpoint = None\n",
    "        logger.warning(\"No checkpoint found. Training from scratch.\") \n",
    "\n",
    "trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0eadf",
   "metadata": {},
   "source": [
    "## Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74d57f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# * evaluating the model on the test set\n",
    "logger.debug(\"evaluating the model on the test set: run 1\")\n",
    "x = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "\n",
    "logger.debug(\"evaluation complete\")\n",
    "logger.info(f\"Results: {x}\")\n",
    "\n",
    "# * testing the model\n",
    "from transformers import pipeline\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "logger.debug(\"Creating the pipeline\")\n",
    "if USING_GPU:\n",
    "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=0, batch_size=16)\n",
    "else:\n",
    "    nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "logger.debug(\"Pipeline created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbe80a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# * formatting the NER results\n",
    "def format_ner_results(ner_results, model=\"xlmr\"):\n",
    "    \"\"\"\n",
    "    Format the NER results to be used for evaluation\n",
    "\n",
    "    Args:\n",
    "    ner_results (list of dictionaries): The NER results containing word and entity information.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two lists - morphs and tags. Morphs is a list of morphemes extracted from the NER results, and tags is a list of corresponding entity tags.\n",
    "\n",
    "    Example:\n",
    "    >>> ner_results = [\n",
    "            {\"word\": \"U\", \"Entity\": \"NPrePre15\"},\n",
    "            {\"word\": \"ku\", \"Entity\": \"BPre15\"},\n",
    "            {\"word\": \"eng\", \"Entity\": \"VRoot\"},\n",
    "            {\"word\": \"##ez\", \"Entity\": \"VRoot\"},\n",
    "            {\"word\": \"a\", \"Entity\": \"VerbTerm\"}\n",
    "        ]\n",
    "    >>> format_ner_results(ner_results)\n",
    "    ([\"u\", \"ku\", \"engez\", \"a\"], [\"NPrePre15\", \"BPre15\", \"VRoot\", \"VerbTerm\"])\n",
    "    \"\"\"\n",
    "    morphs = []\n",
    "    tags = []\n",
    "\n",
    "    if model==\"xlmr\":\n",
    "        for i in range(len(ner_results)):\n",
    "            morph = ner_results[i][\"word\"]\n",
    "            tag = ner_results[i][\"entity\"]\n",
    "\n",
    "            if morph.startswith(\"▁\"):\n",
    "                morphs.append(morph[1:])\n",
    "                if \"Dem\" in tag:\n",
    "                    continue\n",
    "                tags.append(tag)\n",
    "            else:\n",
    "                morphs[-1] += morph\n",
    "    elif model==\"bert\":\n",
    "        for i in range(len(ner_results)):\n",
    "            morph = ner_results[i][\"word\"]\n",
    "            tag = ner_results[i][\"entity\"]\n",
    "\n",
    "            if morph.startswith(\"##\"):\n",
    "                morphs[-1] += morph[2:]\n",
    "            else:\n",
    "                morphs.append(morph)\n",
    "                if \"Dem\" in tag:\n",
    "                    continue\n",
    "                tags.append(tag)\n",
    "    \n",
    "    return morphs, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da272a2a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# * predicting the tags for the test set\n",
    "test_set = lang_set[\"test\"]\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "logger.debug(\"Predicting the tags for the test set\")\n",
    "for i in range(len(test_set)):\n",
    "    sentence = \" \".join(test_set[i][\"morpheme\"])\n",
    "    ner_results = nlp(sentence)\n",
    "    morphs, tags = format_ner_results(ner_results)\n",
    "    expected_tags = [\"_-\" + mappings_r[x] for x in test_set[i][\"tag\"]]\n",
    "    tags = [\"_-\" + x for x in tags]\n",
    "    if len(expected_tags) != len(tags):\n",
    "        continue\n",
    "    predictions.append(tags)\n",
    "    references.append(expected_tags)\n",
    "\n",
    "logger.debug(\"Predictions complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb622df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * evaluating the model on the classification report of the test set\n",
    "\n",
    "logger.debug(\"Evaluating the model on the test set: run 2\")\n",
    "metric = None\n",
    "if args.metric == \"all\":\n",
    "    metric = classification_report\n",
    "elif args.metric == \"f1\":\n",
    "    metric = f1_score\n",
    "elif args.metric == \"precision\":\n",
    "    metric = precision_score\n",
    "elif args.metric == \"recall\":\n",
    "    metric = recall_score\n",
    "\n",
    "results = metric(references, predictions)\n",
    "logger.info(f\"Results: {results}\")\n",
    "\n",
    "logger.debug(\"Evaluation complete\")\n",
    "logger.debug(\"Script complete\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
