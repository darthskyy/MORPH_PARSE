<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MorphParse: Deep Learning for Morphological Parsing</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" href="files/uctlogo.png" type="image/x-icon">
    <link href="files/http_cdn.jsdelivr.net_npm_bootstrap@5.3.3_dist_css_bootstrap.css" rel="stylesheet">
</head>
<body>

<header>
    <div class="uct-logo">
        <img src="files/uctlogo.png" alt="University of Cape Town logo" class="uct-logo">
    </div>
    <div id="site-title">
        <strong>MorphParse</strong>
        <span id="site-subtitle">Deep Learning for Morphological Parsing</span>
    </div>
    <div class="divider"></div>
    <nav>
        <ul>
            <li><a href="#abstract">Abstract</a></li>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#from-scratch">Models Trained From Scratch</a></li>
            <li><a href="#plms">Pre-trained Language Models</a></li>
            <!-- <li><a href="#experimental-setup">Experimental Setup</a></li> -->
            <li><a href="#outcomes">Outcomes</a></li>
        </ul>
    </nav>

    <button id="hamburger" aria-label="Menu">
    <span class="material-icons"><svg xmlns="http://www.w3.org/2000/svg" width="32px" height="32px" viewBox="0 0 24 24">
        <path fill="currentColor" d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path>
    </svg></span>
    </button>
</header>

<div>

</div>
<div id="main-wrap">
    <main>
        <section id="demo-video">
            <video autoplay loop playsinline muted controls>
                <source src="files/demo.mp4" type="video/mp4">

                Your browser does not support playing embedded videos - download the demo video
                <a href="files/demo.mp4" target="_blank">here</a> instead.
            </video>
        </section>

        <section id="abstract">
            <h1>Abstract</h1>
            <p>
                <dfn>Morphemes</dfn> are the smallest units of meaning in a language. They play an important role in creating
                meaning and grammatical syntax. This is especially true in agglutinative languages, such as the Nguni languages,
                which construct words by concatenating many morphemes. Many Natural Language Processing (NLP) tasks, such as
                machine translation, can be improved by incorporating morphological information. However, methods to extract
                this information for the Nguni languages still need to be refined.

            <p>
                This paper evaluates the use of neural  methods in morphological parsing, or the labelling of morphemes
                with their corresponding grammatical role. We compare two main approaches:
                <ul>
                    <li><a href="#from-scratch">Training neural models from scratch</a>, and</li>
                    <li><a href="#plms">Fine-tuning pre-trained language models.</a></li>
                </ul>

            <p>We <a href="#outcomes">compared the performance</a> of these models with
                eachother as well as with traditional methods of solving the task, such as Finite State Transducer (FST) models.

            <p>
                We found that models trained from scratch outperformed both fine-tuned pre-trained language models and the
                traditional FST model. Models using morpheme-level embeddings and sentence-level context tended to perform the
                best.
        </section>

        <section id="introduction">
            <h1>Introduction</h1>

            <p>
                Morphemes are the basic building blocks of meaning (semantics) in a language. By understanding the
                grammatical role that morphemes play in a sentence, we can solve downstream Natural Language Processing
                (NLP) tasks better. These tasks range from information retrieval to machine translation.
            </p>

            <p>
                Splitting up text into its morphemes is known as <dfn>morphological segmentation.</dfn> There are two
                kinds of morphological segmentation: canonical and surface segmentation. <dfn>Canonical segmentation</dfn>
                splits the text into the full, underlying linguistic morphemes in their canonical forms. <dfn>Surface
                segmentation</dfn> splits the text into its <dfn>morphs</dfn>, which are the morphemes as they appear
                in the word after undergoing any sound or spelling changes that may occur.
            </p>

            <figure class="wide-figure">
                <img src="files/seg-example.svg" alt="Canonical and surface segmentations of 'zobomi'. The canonical
                form is 'za-u-(bu)-bomi', whilst the surface form is 'zo-bomi'">
                <figcaption>The surface and canonical segmentations of the word "zobomi" (of life in IsiXhosa).</figcaption>
            </figure>

            <p>
                <dfn id="morph-parse-dfn">Morphological parsing</dfn> is the task of identifying the grammatical role
                of each morpheme within a word.  For example, "zobomi" (meaning "of life" in isiXhosa) is parsed as
                "za[PossConc14] - u[NPrePre14] - (bu)[BPre14] - bomi[NStem]"
                The goal of morphological parsing is to predict these morpheme tags for arbitrary text. In our project
                we focus on the tagging step, and text that is already segmented by another algorithm.
            </p>

            <figure>
                <img src="files/parse-example.svg" alt="Architecture diagram depicting how the word 'zobomi' is handled
                by the parsing system. It is first segmented into za-u-(bu)-bomi by the segmenter. Then, it is passed
                into MorphParse (our models), which produces the tagged output za[PossConc14]-u[NPrePre14]-(bu)[BPre14]-bomi[NStem]">

                <figcaption>Each bracketed tag labels the preceding morpheme with its grammatical function and noun class. The
                morpheme "bomi" is the word's <em>noun stem</em> in this example.
                </figcaption>
            </figure>

            <p>
                The <dfn>Nguni</dfn> languages are a group of widely-spoken South African languages, which include
                IsiXhosa, IsiZulu, IsiNdebele, and SiSwati. These languages are <dfn>low-resourced</dfn>, meaning that
                there are few tools and <dfn>corpora</dfn> (collections of documents) available for these languages.
            </p>

            <figure>
                <img src="files/nguni-speakers.svg" alt="A map of South Africa showing what proportion
                of people speak a Nguni language at home. Speakership is most dominant in the south-east of the country,
                covering almost the whole of the Eastern Cape and Kwa-Zulu Natal, but stretching into Gauteng and
                Mpumalanga.">
                <figcaption>Proportion of South Africans who speak a Nguni language at home.
                    <a href="https://en.wikipedia.org/wiki/File:South_Africa_2011_Nguni_speakers_proportion_map.svg">(Original
                    on Wikipedia</a>, public domain)
                </figcaption>
            </figure>

            <p>
                Morphological information is especially important for NLP in Nguni languages for two main reasons:
                <ul>
                    <li>
                        Nguni languages are <dfn>agglutinative</dfn>, meaning many words are created by combining
                        multiple morphemes.
                    </li>

                    <li>
                        Nguni languages are written conjunctively, meaning that morphemes are concatenated into a single
                        word. For example, in isiXhosa, "andikambuzi" means "I haven't yet asked him", and is
                        composed of the morphemes "a", "ndi", "ka", "m", "buza", and "i".
                    </li>
                </ul>

            <p>
                Few morphological parsers exist for the Nguni languages. One example of a morphological parser is the
                rule-based <a href="https://portal.sadilar.org/FiniteState/demo/zulmorph/">ZulMorph parser for isiZulu.</a>
                Rule-based parsers require linguists to manually incorporate stems, affixes, and grammar rules into the
                software. This is a tedious process which requires a high degree of expertise.

            <p>
                By comparison, machine-learning approaches are data driven. Instead of manually incorporating information
                into the algorithm, the parser can be automatically generated from linguistically-annotated data. This
                means that the process is language agnostic and can leverage previously-existing datasets.

            <p>
                In this project, we investigated the use of neural methods for morphological parsing of Nguni languages.
                We took two main approaches to this:
                <ul>
                    <li>
                        Simbarashe Mawere examined the fine-tuning of pre-trained language models. Three different
                        pre-trained language models (PLMs) were evaluated, with varying levels of inclusion of Nguni
                        Languages. These were XLM-Roberta (XLM), Afro-XLMR, and Nguni-XLMR.
                    </li>
                    <li>
                        Cael Marquard examined the use of training models from scratch. Two kinds of models were evaluated:
                        bidirectional Long Short-Term Memory (bi-LSTM) models, and neural Conditional Random Fields (CRFs).
                    </li>
                </ul>

            <p>
                The three research questions that we aimed to answer were:
                <ol>
                    <li><strong>Can neural approaches outperform traditional approaches to morphological parsing for the Nguni
                    languages?</strong></li>
                    <li><strong>Do models trained from scratch or fine-tuned pre-trained language models perform better?</strong></li>
                    <li><strong>Do models classifying surface segmentations or models classifying canonical segmentations
                    perform better?</strong></li>
                </ol>

            <p>
                In order to answer these questions, we compared the tagging quality of these models to each-other and
                to a traditional, rule-based approach
                (<a href="https://portal.sadilar.org/FiniteState/demo/zulmorph/">ZulMorph</a>) as a baseline. Comparisons
                were also made across segmentation type.
        </section>

        <!-- TODO separate aims maybe? -->
        <!-- TODO methodology -->

        <section id="from-scratch">
            <h1>Models Trained From Scratch</h1>
            <p class="author">Author: Cael Marquard</p>

            <h2>Models</h2>
            <p>
                Two architectures were chosen: Bidirectional Long Short-Term Memory (Bi-LSTM) and
                neural Conditional Random Fields (CRFs). These architectures have both been successfully applied to the
                closely-related tasks of morphological segmentation and part-of-speech tagging for the Nguni languages.

            <h3>Bi-LSTMs</h3>
            <p>
                <dfn>Bi-LSTM</dfn> models are a type of <dfn>recurrent neural network</dfn> (RNN), a class of
                neural networks able to "remember" past inputs when computing future outputs. LSTMs are an
                RNN architecture which avoid issues such as vanishing and exploding gradients, and are popular for
                NLP tasks. Bidirectional LSTMs combine two separate LSTMs, one reading the input forward and one in
                reverse. This allows bi-LSTMs to take into account both the future <em>and</em> the past of the sequence
                that it is classifying in order to have better context.
            </p>

            <h3>CRFs</h3>

            <p>
                A <dfn>CRF</dfn> is a probabilistic model which explicitly models the statistical dependence of the output
                (label) sequence on the input sequence, as <em>well</em> as the dependence of the output sequence
                <em>on itself.</em> This allows them to explicitly model the grammar of the language. CRFs have been used
                for morphological segmentation as well as part-of-speech tagging in the Nguni languages. For this project,
                the CRF only models the interdependence of each label on the neighbouring labels and the input item.
                This linear chain approach is simpler to implement and more computationally efficient to train.

            <figure class="wide-figure">
                <img src="files/crf.png" alt="Diagram of a CRF organised as a linear chain. Each output label depends on
                 the previous and next output labels, as well as the input datum which it labels.">
                <figcaption>An example of a CRF organised as a linear chain. Diamonds represent input variables
                    <strong>X<sub>i</sub></strong>, circles represent output variables
                    <strong>Y<sub>i</sub></strong>, and edges represent statistical interdependence.
                </figcaption>
            </figure>

            <p>CRFs usually rely on a set of hand-crafted features in order to assign probabilities. However, an
                alternative to this is to use a neural network to generate these features. In this project, a bi-LSTM
                is used to generate these features.
        </section>

        <section id="plms">
            <h1>Pre-trained Language Models</h1>
            <p class="author">Author: Simbarashe Mawere</p>
            <p><dfn>Pre-trained language models</dfn> (PLMs) are transformer based models that have been trained on large corpora of text, allowing them to learn the structure of language. They make use of masked language modelling (MLM) as the pre-training task from which the models gain the pre-learned word contexts and embeddings. This is a task where a word is masked and the model is task on predicting what it was. After this the model can be fine-tuned on data for a different task with considerable performance at reduced time and resource expenditure. <br><br>

            Beginning with BERT in there have been many models that have been developed and fine-tuned for different languages and tasks. These models have been shown to be effective in a wide range of NLP tasks and have been shown to be effective in low-resource languages. In this exploration we fine-tuned three models on the morphological parsing task for the Nguni languages. The models were XLM-RoBERTa, Afro-XLMR and Nguni-XLMR based on their varying levels of inclusion of the Nguni languages. <br><br>
            </p>
            
            <figure class="wide-figure">
                <img src="files/bert working.png" alt="">
                <figcaption>Illustration of how BERT works. The model is trained on a masked language modelling task, where it predicts masked words in a sentence. This allows it to learn the structure of language and later use that knowledge when being fine-tuned on more down-stream tasks.</figcaption>
            </figure>
            <br><hr><br>
            <h2>Models</h2>
            <div id="modelsCarousel" class="carousel carousel-dark slide" data-bs-ride="true">
                <div class="carousel-indicators">
                    <button type="button" data-bs-target="#modelsCarousel" data-bs-slide-to="0" class="active" aria-current="true" aria-label="Slide 1"></button>
                    <button type="button" data-bs-target="#modelsCarousel" data-bs-slide-to="1" aria-label="Slide 2"></button>
                    <button type="button" data-bs-target="#modelsCarousel" data-bs-slide-to="2" aria-label="Slide 3"></button>
                </div>
                <div class="carousel-inner">
                    <div class="carousel-item active">
                        <div class="d-flex justify-content-center">
                            <div class="card w-75" style="height: 390px;">
                                <div class="card-body">
                                    <h3 class="card-title">XLM-RoBERTa</h3>
                                    <p class="card-text">XLM-RoBERTa is a model that was proposed by <a href="https://arxiv.org/abs/1911.02116">Conneau et al.</a> to be pre-trained on 100 languages and applied on various cross-lingual transfer tasks like cross-lingual natural language inference (XNLI) and named entity recognition (NER).</p>
                                    <p><strong>Strength:</strong> High multilingual capabilities with strong results across different tasks and good performance for low-resource languages.</p>
                                    <p><strong>Included Nguni Languages:</strong> isiXhosa</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="carousel-item">
                        <div class="d-flex justify-content-center">
                            <div class="card w-75" style="height: 390px;">
                                <div class="card-body">
                                    <h3 class="card-title">Afro-XLMR</h3>
                                    <p class="card-text">Afro-XLMR is a version of XLM-RoBERTa fine-tuned using multilingual adaptive fine-tuning (MAFT) on 20 African languages by <a href="https://aclanthology.org/2024.eacl-long.14/#:~:text=In%20this%20paper%2C%20we%20create,Natural%20Language%20Understanding%20(NLU).">Alabi et al.</a> to improve performance on languages that were previously unseen in XLM-R's pre-training corpora.</p>
                                    <p><strong>Strengths:</strong> Specialized for African languages with improved performance for low-resource languages and distilled size from XLM-R</p>
                                    <p><strong>Included Nguni Languages:</strong> isiXhosa & isiZulu</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="carousel-item">
                        <div class="d-flex justify-content-center">
                            <div class="card" style="height: 390px; width: 700px;">
                                <div class="card-body">
                                    <h3 class="card-title">Nguni-XLMR</h3>
                                    <p class="card-text">Nguni-XLMR by <a href="https://aclanthology.org/2024.lrec-main.1071/">Meyer et al.</a> follow the multilingual adaptive fine-tuning as in Afro-XLMR but on a narrower linguistic scope to leverage the similarity of the Nguni languages for cross-lingual transfer. This model was evaluate on a variety of natural language understanding tasks and showed improved over the two other PLMs.</p>
                                    <p><strong>Strength:</strong> Optimized for Nguni languages, offering superior results in linguistic tasks.</p>
                                    <p><strong>Included Nguni Languages:</strong> all</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
                <!-- Carousel Controls -->
                <button class="carousel-control-prev" type="button" data-bs-target="#modelsCarousel" data-bs-slide="prev">
                    <span class="carousel-control-prev-icon" aria-hidden="true"></span>
                    <span class="visually-hidden">Previous</span>
                </button>
                <button class="carousel-control-next" type="button" data-bs-target="#modelsCarousel" data-bs-slide="next">
                    <span class="carousel-control-next-icon" aria-hidden="true"></span>
                    <span class="visually-hidden">Next</span>
                </button>
            </div>
        </section>

        <!-- <section id="experimental-setup">
            <h1>Experimental Setup</h1>
            <h2>Resources</h2>
            <p>
            Given the nature of the project being experimental, there was need for both computational and data resources. The following resources were used in the project:
            </p>
            <h3>Computational Resources</h3>
            <p>We made use of the National Integrated Cyberinfrastructure System's Centre for High-Performance Computing (NICIS CHPC) GPU cluster to train our models since the training runs were very computationally expensive. This was especially true for the PLMs which have hundreds of millions of paremeters. The main GPUs used were single NVIDIA V100 GPU cores which could be reserved for 12 hours of walltime at a time on the cluster.</p>
            <h3>Dataset</h3>
            <p>The dataset used in the project was the Sadilar dataset which consists of four parallel sets of data across all four Nguni languages. The data is from a collection of selected documents from South African governmnet websites. Each line consisted of a word, and its morphological parsing, lemma and part of speech. We took the morphological analysis and made input and output sequences for consisting of morphemes and their tags respectively.</p>

            <h2>Evaluation</h2>
            <p>
                The models were evaluated using the <dfn>F1 score</dfn>, which is the harmonic mean of the <dfn>precision</dfn>
                and <dfn>recall</dfn> of the model. The F1 score is a common metric for evaluating the performance of
                classifiers, and is especially useful when the classes are imbalanced. The F1 score is calculated as:
                <blockquote>
                    F1 = 2 * (precision * recall) / (precision + recall)
                </blockquote>
                Where:
                <ul>
                    <li><dfn>Precision</dfn> is the number of true positives divided by the number of true positives plus
                        the number of false positives. It is a measure of how many of the predicted positive morphological tags were
                        actually positive.
                    </li>
                    <li><dfn>Recall</dfn> is the number of true positives divided by the number of true positives plus the
                        number of false negatives. It is a measure of how many of the actual positive morphological tags were predicted
                        as positive.
                    </li>
                </ul>
            </p>
        </section>
         -->

        <section id="outcomes">
            <h1>Outcomes</h1>

            <h2>Results</h2>
            <p>The results for the exploration were split based on the data which the models were trained on. The first
                exploration in the project was using expertly annotated canonical segmentations which were obtained from
                the Gaustad & Puttkammer's (2022) dataset. For the other result sets we used canonical and surface
                segmentations predicted using the morphological segmenters from MORPH-SEGMENT. For the models trained
                from scratch the models are split into word- and sentence-level.</p>

            <p>Click to view the full tables of results:</p>
            <div id="extended-results">
                <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#gold-data">
                    Gold standard canonical segmentations
                </button>

                <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#gold-surf-data">
                    Gold standard surface segmentations
                </button>

                <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#e2e-pred-canon">
                    End-to-end on predicted canonical segmentations
                </button>

                <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#e2e-surface-pred">
                    End-to-end on predicted surface segmentations
                </button>
            </div>

            <div class="modal fade modal-xl" id="gold-data" tabindex="-1" aria-labelledby="gold-data-label" aria-hidden="true">
              <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="gold-data-label">Results on Gold Standard Canonical Segmentations</h5>
                    <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                  </div>
                  <div class="modal-body">
                    <table id="gold-table">
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th colspan="2">isiNdebele</th>
                                <th colspan="2">siSwati</th>
                                <th colspan="2">isiXhosa</th>
                                <th colspan="2">isiZulu</th>
                            </tr>
                            <tr class="score-heading">
                                <th></th>
                                <th>Micro F1</th>
                                <th>Macro F1</th>
                                <th>Micro F1</th>
                                <th>Macro F1</th>
                                <th>Micro F1</th>
                                <th>Macro F1</th>
                                <th>Micro F1</th>
                                <th>Macro F1</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <th colspan="9">Baselines</th>
                            </tr>
                            <tr>
                                <td>ZulMorph</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.6471</td><td>0.3378</td>
                            </tr>
                            <tr>
                                <th colspan="9">Models Trained from Scratch</th>
                            </tr>
                            <tr>
                                <td>Bi-LSTM, morpheme</td><td>0.9222</td><td>0.6950</td><td>0.9160</td><td>0.6761</td><td>0.9529</td><td>0.7359</td><td>0.9264</td><td class="best"><u>0.6860</u></td>
                            </tr>
                            <tr>
                                <td>Bi-LSTM, char-sum</td><td class="best"><u>0.9226</u></td><td>0.6907</td><td>0.9177</td><td>0.6835</td><td>0.9553</td><td>0.7418</td><td>0.9271</td><td>0.6817</td>
                            </tr>
                            <tr>
                                <td>Bi-LSTM, morpheme</td><td>0.9188</td><td>0.7009</td><td>0.9190</td><td>0.6872</td><td>0.9609</td><td>0.7694</td><td>0.9263</td><td>0.6812</td>
                            </tr>
                            <tr>
                                <td>Bi-LSTM, char-sum</td><td>0.9142</td><td>0.6901</td><td>0.9132</td><td>0.6748</td><td>0.9585</td><td>0.7604</td><td>0.9210</td><td>0.6661</td>
                            </tr>
                            <tr>
                                <td>CRF, morpheme</td><td>0.9189</td><td class="best"><u>0.7047</u></td><td class="best"><u>0.9196</u></td><td class="best"><u>0.6945</u></td><td>0.9619</td><td class="best"><u>0.7777</u></td><td class="best"><strong>0.9272</strong></td><td>0.6825</td>
                            </tr>
                            <tr>
                                <td>CRF, char-sum</td><td>0.9167</td><td>0.7007</td><td>0.9179</td><td>0.6855</td><td class="best"><u>0.9623</u></td><td>0.7633</td><td>0.9255</td><td>0.6730</td>
                            </tr>
                            <tr>
                                <th colspan="9">Pre-trained Language Models</th>
                            </tr>
                            <tr>
                                <td>XLM-RoBERTa</td><td class="best"><strong>0.9152</strong></td><td class="best"><strong>0.6425</strong></td><td>0.9095</td><td>0.6420</td><td>0.9467</td><td>0.6773</td><td>0.9132</td><td>0.6157</td>
                            </tr>
                            <tr>
                                <td>Afro-XLMR</td><td>0.9133</td><td>0.6273</td><td class="best"><strong>0.9100</strong></td><td class="best"><strong>0.6460</strong></td><td class="best"><strong>0.9583</strong></td><td class="best"><strong>0.7363</strong></td><td class="best"><u>0.9282</u></td><td class="best"><strong>0.6610</strong></td>
                            </tr>
                            <tr>
                                <td>Nguni-XLMR</td><td>0.9104</td><td>0.6190</td><td>0.9042</td><td>0.6176</td><td>0.9488</td><td>0.6738</td><td>0.9187</td><td>0.6302</td>
                            </tr>
                        </tbody>
                    </table>
                  </div>
                </div>
              </div>
            </div>

            <div class="modal fade modal-xl" id="gold-surf-data" tabindex="-1" aria-labelledby="gold-surf-label" aria-hidden="true">
              <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="gold-surf-label">Results on Gold Standard Surface Segmentations</h5>
                    <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                  </div>
                  <div class="modal-body">
                    <table id="gold-surf-table">
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th colspan="2">isiNdebele</th>
                                <th colspan="2">siSwati</th>
                                <th colspan="2">isiXhosa</th>
                                <th colspan="2">isiZulu</th>
                            </tr>
                            <tr class="score-heading">
                                <th></th>
                                <th>Micro F1</th>
                                <th>Macro F1</th>
                                <th>Micro F1</th>
                                <th>Macro F1</th>
                                <th>Micro F1</th>
                                <th>Macro F1</th>
                                <th>Micro F1</th>
                                <th>Macro F1</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <th colspan="9">Models Trained from Scratch</th>
                            </tr>
                            <tr>
                                <td>Bi-LSTM, morpheme</td><td class="best"><u>0.911</u></td><td class="best"><u>0.677</u></td><td class="best"><u>0.905</u></td><td class="best"><u>0.657</u></td><td class="best"><u>0.952</u></td><td class="best"><u>0.767</u></td><td class="best"><u>0.912</u></td><td class="best"><u>0.659</u></td>
                            </tr>
                            <tr>
                                <td>Bi-LSTM, char-sum</td><td>0.903</td><td>0.650</td><td>0.894</td><td>0.613</td><td>0.903</td><td>0.747</td><td>0.907</td><td>0.626</td>
                            </tr>
                        </tbody>
                    </table>
                  </div>
                </div>
              </div>
            </div>

            <div class="modal fade modal-xl" id="e2e-pred-canon" tabindex="-1" aria-labelledby="e2e-pred-canon-label" aria-hidden="true">
              <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="e2e-pred-canon-label">End-to-End Results on Predicted Canonical Segmentations</h5>
                    <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                  </div>
                  <div class="modal-body">
                    <table id="canonical-table">
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th colspan="2">isiNdebele</th>
                                <th colspan="2">siSwati</th>
                                <th colspan="2">isiXhosa</th>
                                <th colspan="2">isiZulu</th>
                            </tr>
                            <tr class="score-heading">
                                <th></th>
                                <th>Micro F<sub>1</sub></th>
                                <th>Macro F<sub>1</sub></th>
                                <th>Micro F<sub>1</sub></th>
                                <th>Macro F<sub>1</sub></th>
                                <th>Micro F<sub>1</sub></th>
                                <th>Macro F<sub>1</sub></th>
                                <th>Micro F<sub>1</sub></th>
                                <th>Macro F<sub>1</sub></th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <th colspan="9">Baselines</th>
                            </tr>
                            <tr>
                                <td>ZulMorph</td>
                                <td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>0.6471</td><td>0.3378</td>
                            </tr>

                            <!-- <tr><td colspan="9"><hr></td></tr> -->

                            <tr>
                                <th colspan="9">Models Trained from Scratch</th>
                            </tr>
                            <tr>
                                <td>Bi-LSTM, morpheme</td>
                                <td>0.8084</td><td>0.5769</td><td>0.8230</td><td>0.5717</td><td>0.9110</td><td>0.6807</td><td>0.8250</td><td>0.5936</td>
                            </tr>
                            <tr>
                                <td>Bi-LSTM, char-sum</td>
                                <td class="best"><strong>0.8094</strong></td><td class="best"><u>0.6816</u></td><td>0.8289</td><td>0.5760</td><td>0.9107</td><td>0.6816</td><td>0.8248</td><td class="best"><u>0.6033</u></td>
                            </tr>

                            <tr>
                                <td>Bi-LSTM, morpheme<br>(Sentence-level)</td>
                                <td>0.8059</td><td>0.5834</td><td>0.8274</td><td>0.5792</td><td>0.9172</td><td>0.7171</td><td>0.8265</td><td>0.5989</td>
                            </tr>
                            <tr>
                                <td>Bi-LSTM, char-sum<br>(Sentence-level)</td>
                                <td>0.8010</td><td>0.5814</td><td>0.8238</td><td>0.5749</td><td>0.9135</td><td>0.7020</td><td>0.8184</td><td>0.5903</td>
                            </tr>
                            <tr>
                                <td>CRF, morpheme<br>(Sentence-level)</td>
                                <td>0.8049</td><td>0.5961</td><td class="best"><u>0.8301</u></td><td class="best"><u>0.5860</u></td><td class="best"><u>0.9190</u></td><td class="best"><u>0.7224</u></td><td class="best"><u>0.8280</u></td><td>0.5991</td>
                            </tr>
                            <tr>
                                <td>CRF, char-sum<br>(Sentence-level)</td>
                                <td>0.8081</td><td>0.5893</td><td>0.8271</td><td>0.5782</td><td>0.9181</td><td>0.7084</td><td>0.8246</td><td>0.5973</td>
                            </tr>

                            <tr>
                                <th colspan="9">Pre-trained Language Models</th>
                            </tr>
                            <tr>
                                <td>XLM-RoBERTa</td>
                                <td class="best"><u>0.8151</u></td><td class="best"><strong>0.5509</strong></td><td class="best"><strong>0.8280</strong></td><td>0.5278</td><td>0.9137</td><td>0.6346</td><td>0.8251</td><td>0.5438</td>
                            </tr>
                            <tr>
                                <td>Afro-XLMR</td>
                                <td>0.8137</td><td>0.5413</td><td>0.8273</td><td class="best"><strong>0.5296</strong></td><td class="best"><strong>0.9140</strong></td><td class="best"><strong>0.6423</strong></td><td>0.8269</td><td>0.5469</td>
                            </tr>
                            <tr>
                                <td>Nguni-XLMR</td>
                                <td>0.8144</td><td>0.5468</td><td>0.8264</td><td>0.5285</td><td>0.9155</td><td>0.6390</td><td class="best"><strong>0.8272</strong></td><td class="best"><strong>0.5495</strong></td>
                            </tr>
                        </tbody>
                    </table>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
                    <button type="button" class="btn btn-primary">Save changes</button>
                  </div>
                </div>
              </div>
            </div>

            <div class="modal fade modal-xl" id="e2e-surface-pred" tabindex="-1" aria-labelledby="e2e-surface-pred-label" aria-hidden="true">
              <div class="modal-dialog">
                <div class="modal-content">
                  <div class="modal-header">
                    <h5 class="modal-title" id="e2e-surface-pred-label">End-to-End Results on Predicted Surface Segmentations</h5>
                    <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                  </div>
                  <div class="modal-body">
                    <table>
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th colspan="2">isiNdebele</th>
                                <th colspan="2">siSwati</th>
                                <th colspan="2">isiXhosa</th>
                                <th colspan="2">isiZulu</th>
                            </tr>
                            <tr class="score-heading">
                                <th></th>
                                <th>Micro F<sub>1</sub></th>
                                <th>Macro F<sub>1</sub></th>
                                <th>Micro F<sub>1</sub></th>
                                <th>Macro F<sub>1</sub></th>
                                <th>Micro F<sub>1</sub></th>
                                <th>Macro F<sub>1</sub></th>
                                <th>Micro F<sub>1</sub></th>
                                <th>Macro F<sub>1</sub></th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <th colspan="9">Models Trained from Scratch</th>
                            </tr>
                            <tr>
                                <td>Bi-LSTM, morpheme</td><td class="best"><strong><u>0.7830</u></strong></td><td class="best"><strong><u>0.5409</u></strong></td><td class="best"><strong><u>0.8129</u></strong></td><td class="best"><strong><u>0.5274</u></strong></td><td class="best"><strong><u>0.8661</u></strong></td><td class="best"><strong><u>0.6724</u></strong></td><td class="best"><strong><u>0.8023</u></strong></td><td class="best"><strong><u>0.5529</u></strong></td>
                            </tr>
                            <tr>
                                <td>Bi-LSTM, char-sum</td><td>0.7742</td><td>0.5238</td><td>0.8053</td><td>0.5212</td><td>0.7994</td><td>0.6054</td><td>0.7976</td><td>0.5511</td>
                            </tr>
                            <tr>
                                <th colspan="9">Pre-trained Language Models</th>
                            </tr>
                            <tr>
                                <td>XLM-RoBERTa</td><td class="best"><strong>0.7282</strong></td><td class="best"><strong>0.4868</strong></td><td>0.5107</td><td>0.2231</td><td>0.7244</td><td>0.5208</td><td>0.6759</td><td>0.4349</td>
                            </tr>
                            <tr>
                                <td>Afro-XLMR</td><td>0.7275</td><td>0.4832</td><td>0.5216</td><td>0.2412</td><td>0.7273</td><td class="best"><strong>0.5300</strong></td><td>0.6794</td><td>0.4495</td>
                            </tr>
                            <tr>
                                <td>Nguni-XLMR</td><td>0.7258</td><td>0.4746</td><td class="best"><strong>0.5328</strong></td><td class="best"><strong>0.2505</strong></td><td class="best"><strong>0.7309</strong></td><td>0.5270</td><td class="best"><strong>0.6818</strong></td><td class="best"><strong>0.4512</strong></td>
                            </tr>
                        </tbody>
                    </table>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Close</button>
                    <button type="button" class="btn btn-primary">Save changes</button>
                  </div>
                </div>
              </div>
            </div>

            <!-- Modal for PLM Lex Analysis Section -->
            <div class="modal fade modal-xl" id="lex-analysis" tabindex="-1" aria-labelledby="lex-analysis-label" aria-hidden="true">
                <div class="modal-dialog">
                  <div class="modal-content">
                    <div class="modal-header">
                      <h5 class="modal-title" id="lex-analysis-label">Lexical Analysis of the Dataset</h5>
                      <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                    </div>
                    <div class="modal-body">
                        <table border="1" cellpadding="10" cellspacing="0">
                            <thead>
                              <tr>
                                <th></th>
                                <th>isiNdebele</th>
                                <th>siSwati</th>
                                <th>isiXhosa</th>
                                <th>isiZulu</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <th>Word count</th>
                                <td>49689</td><td>47385</td><td>48735</td><td>49097</td>
                              </tr>
                              <tr>
                                <th colspan="5" style="text-align:center;">Canonical Segmentations</th>
                              </tr>
                              <tr>
                                <th>Morpheme count</th>
                                <td>137400</td><td>127698</td><td>149294</td><td>144047</td>
                              </tr>
                              <tr>
                                <th>Morphemes/word</th>
                                <td>2.77</td><td>2.69</td><td>3.06</td><td>2.93</td>
                              </tr>
                              <tr>
                                <th>Unique morphemes</th>
                                <td>5100</td><td>3389</td><td>2453</td><td>3284</td>
                              </tr>
                              <tr>
                                <th>Unique tags</th>
                                <td>240</td><td>246</td><td>236</td><td>256</td>
                              </tr>
                              <tr>
                                <th colspan="5" style="text-align:center;">Surface Segmentations</th>
                              </tr>
                              <tr>
                                <th>Morpheme count</th>
                                <td>131204</td><td>125282</td><td>133244</td><td>133476</td>
                              </tr>
                              <tr>
                                <th>Morphemes/word</th>
                                <td>2.64</td><td>2.64</td><td>2.73</td><td>2.72</td>
                              </tr>
                              <tr>
                                <th>Unique morphemes</th>
                                <td>5843</td><td>4932</td><td>3762</td><td>4240</td>
                              </tr>
                              <tr>
                                <th>Unique tags</th>
                                <td>252</td><td>342</td><td>340</td><td>350</td>
                              </tr>
                            </tbody>
                          </table>                          
                    </div>
                  </div>
                </div>
              </div>

            <figure class="wide-figure">
                <img src="files/basic-results.svg" alt="Graph comparing Macro F1 of ZulMorph with Afro-XLMR and
                bi-LSTMs. In all cases, surface-based models are outperformed by their canonical counterparts. The
                bi-LSTM models are best overall.">
                <figcaption>The Macro F<sub>1</sub> scores of three selected models (Afro-XLMR, Bi-LSTM, and ZulMorph).</figcaption>
            </figure>

            Click to view the full tables of results:
            <div id="extended-results">
                <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#gold-data">
                    Gold standard canonical segmentations
                </button>

                <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#gold-surf-data">
                    Gold standard surface segmentations
                </button>

                <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#e2e-pred-canon">
                    End-to-end on predicted canonical segmentations
                </button>

                <button type="button" class="btn btn-primary" data-bs-toggle="modal" data-bs-target="#e2e-surface-pred">
                    End-to-end on predicted surface segmentations
                </button>
            </div>


            <h2>Discussion</h2>

            <div class="discussion">
                <div class="discussion-text">
                    <h3>Models Trained from Scratch</h3>

                    <p><strong>Both Bi-LSTMs and CRFs performed well.</strong> The CRF
                        layer did not improve significantly over the bi-LSTM used to generate its features. This could
                        be due to the CRF being a simple linear chain &mdash; perhaps higher-order
                        CRFs
<!--                         (which model dependence across more than just neighbours) c-->
                        could improve this.</p>
                    <p><strong>Sentence-level was better than word-level.</strong>
                        This makes intuitive sense as the added context allows for easier disambiguation.
<!--                        For instance, it is not  obvious from the prefix alone whether "ipolisa" should be in class 5 or-->
<!--                        class 9, since both have "i" as valid prefixes. However, if this is placed in the context of the-->
<!--                        sentence "ipolisa liyahamba", the  "li" prefix can be used to unambigously identify "ipolisa" as-->
<!--                        class 5.-->
        <!--            <p><strong>Models classifying canonical segmentations outperformed those classifying surface segmentations.</strong>-->
        <!--                This is likely because the canonical segmentation of a word provides more information to the model-->
        <!--                than the surface segmentation. For instance, the word "kwicandelo" is canonically segmented as-->
        <!--                "ku-i-(li)-candelo" and surface segmented as "kw-i-candelo". The "(li)" morpheme is lost in the-->
        <!--                surface form, which is the noun prefix for class 5. Without it, the model may have a harder time-->
        <!--                choosing between class 9 and 5 for the morpheme "i".-->
        <!--            </p>-->
                    <p><strong>Morpheme-level models outperformed character-level models.</strong>
                        This could be because morphemes are a more effective representation, or because
                        morpheme embeddings are more sensitive to small changes in the morpheme.
<!--                        Since each morpheme is mapped to its own learnt embedding, even a single differing character can-->
<!--                        yield an entirely different embedding. This lets the model identify rare classes more easily.-->
                    </p>

                    <h3>Pre-trained Language Models</h3>
                    <p><strong>Effect of Nguni-specific transfer learning.</strong>
                        It was the expectation that the Nguni-XLMR model would outperform the other PLMs due to its specialized pre-training on tasks for the Nguni languages however this was not the case. The model performed well but was not the best in any of the tasks. This could be due to the fact that the model was trained on a narrower linguistic scope than the other PLMs and thus did not have the same level of generalization as the other models. The model was also not able to leverage the similarity of the Nguni languages as effectively as expected. Af
                    </p>
                    <p><strong>Effect of subword tokenization.</strong>
                        Since the models were adapted from XLM-RoBERTa, they all used a SentencePiece tokenizer which is optimised for subword tokenization. This is suboptimal for our task since its inputs are already subword units. By subword tokenizing individual morphemes, we reduced the models ability to learn the morphological structure.
                    </p>
                    <p><strong>Lexical analysis of datasets.</strong>
                        <button data-bs-toggle="modal" data-bs-target="#lex-analysis">Analysis of the dataset</button> revealed that isiXhosa had an advantage over the other languages since it had less unique morphemes and thus the model was able to learn the morphological structure more effectively. It achieve significantly higher (+9%) macro-F1 score over the other languages.
                    </p>
                </div>

                <div class="graph">
                    <img src="files/graph-vertical.svg" alt="Graph comparing Macro F1 of ZulMorph with Afro-XLMR and
                    bi-LSTMs. In all cases, surface-based models are outperformed by their canonical counterparts. The
                    bi-LSTM models are best overall.">
                </div>

            </div>

            <h2>Conclusions</h2>
            <div class="discussion">
                <div class="discussion-text">
                    <p>Our research questions can be answered as follows:</p>
                    <ol>
                        <li>
                            <p class="research-q">Can neural approaches outperform traditional approaches to morphological parsing for the Nguni
                            languages?</p>
                            <p class="research-ans"><strong>Yes.</strong> Our deep-learning approaches (MorphParse) outperformed the rule-based baseline (ZulMorph).</p>
                        </li>
                        <li>
                            <p class="research-q">Do models trained from scratch or fine-tuned pre-trained language models perform better?</p>
                            <p class="research-ans"><strong>Models trained from scratch.</strong> The gap is not huge, though, and
                            this could be due to issues like suitability of the PLM's tokenisers. Both approaches performed well
                            overall and outperformed the baseline by similar margins.</p>
                        </li>
                        <li>
                            <p class="research-q">Do models classifying surface segmentations or models classifying canonical segmentations
                            perform better?</p>
                            <p class="research-ans"><strong>Canonical segmentations.</strong> The models performed significantly
                                better on canonical segmentations than on surface segmentations. This could be because canonical
                                segmentations provide the model with more linguistic information, and tend to segment the text
                                into more morphemes (advantaging our sequence-tagging models).
                            </p>
                        </li>
                    </ol>
                </div>

                <div class="graph vertical-graph">
                    <img src="files/graph-vertical.svg" alt="Graph comparing Macro F1 of ZulMorph with Afro-XLMR and
                    bi-LSTMs. In all cases, surface-based models are outperformed by their canonical counterparts. The
                    bi-LSTM models are best overall.">
                </div>

                <div class="graph horizontal-graph">
                    <figure class="wide-figure">
                        <img src="files/graph-horizontal.svg" alt="Graph comparing Macro F1 of ZulMorph with Afro-XLMR and
                        bi-LSTMs. In all cases, surface-based models are outperformed by their canonical counterparts. The
                        bi-LSTM models are best overall.">
                    </figure>
                </div>
            </div>

            <p>Our project's contributions are as follows:</p>
            <ul>
                <li><strong>We demonstrated the viability of neural methods in morphological parsing for Nguni languages.</strong>
                    Both
                    PLMs and models trained from scratch performed well at the task, showing that the approach is viable.
                </li>
                <li><strong>We developed new state-of-the-art morphological taggers for Nguni languages.</strong> The morphological taggers that we
                have developed outperform the previous baseline.</li>
            </ul>

            <h2>Deliverables</h2>
            <div class="exploration-content">
                <div class="pdf-item">
                    <h3>Research Paper - Simbarashe Mawere</h3>
                    <embed class="pdf-embed" src="files/MWRSIM003_MorphParse_Final_Report_site_version.pdf" type="application/pdf"
                        title="Simbarashe Mawere's final report, dealing with pre-trained language models (PDF)">
                </div>
                <div class="pdf-item">
                    <h3>Literature Review - Simbarashe Mawere</h3>
                    <embed class="pdf-embed" src="files/MWRSIM003_MorphParse_Literature_Review_site_version.pdf" type="application/pdf"
                        title="Simbarashe Mawere's extended literature review (PDF)">
                </div>
            </div>

            <div class="exploration-content">
                <div class="pdf-item">
                    <h3>Research Paper - Cael Marquard</h3>
                    <embed class="pdf-embed" src="files/MRQCAE001_MorphParse_report.pdf" type="application/pdf"
                           title="Cael Marquard's final report, dealing with models trained from scratch (PDF)">
                </div>
                <div class="pdf-item">
                    <h3>Literature Review - Cael Marquard</h3>
                    <embed class="pdf-embed" src="files/MRQCAE001_MorphParse_literature_review.pdf" type="application/pdf"
                        title="Cael Marquard's extended literature review (PDF)">
                </div>
            </div>

            <div class="exploration-content">
                <div class="pdf-item">
                    <h3>Proposal</h3>
                    <embed class="pdf-embed" src="files/MorphParse%20Project%20Proposal%20Final.pdf" type="application/pdf"
                           title="Joint project proposal">
                </div>
                <div class="pdf-item">
                    <h3>Poster</h3>
                    <embed class="pdf-embed" src="files/poster.pdf" type="application/pdf"
                        title="Cael Marquard's extended literature review (PDF)">
                </div>
            </div>

        </section>
    </main>
</div>

<footer>
    <p>&copy; 2024 MorphParse | By Simbarashe Mawere and Cael Marquard</p>
</footer>

<script src="script.js"></script>
<script src="files/http_cdn.jsdelivr.net_npm_bootstrap@5.3.3_dist_js_bootstrap.bundle.js"></script>
</body>
</html>
