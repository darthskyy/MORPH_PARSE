<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MorphParse: Deep Learning for Morphological Parsing</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="icon" href="files/uctlogo.png" type="image/x-icon">
</head>
<body>

<header>
    <div class="uct-logo">
        <img src="files/uctlogo.png" alt="University of Cape Town logo" class="uct-logo">
    </div>
    <nav>
        <div id="site-title">
            <strong>MorphParse</strong>
            <span id="site-subtitle">Deep Learning for Morphological Parsing</span>
        </div>
        <ul>
            <li><a href="#abstract">Abstract</a></li>
            <li><a href="#introduction">Background</a></li>
            <li><a href="#from-scratch">Models Trained From Scratch</a></li>
            <li><a href="#plms">Pre-trained Language Models</a></li>
            <li><a href="#results">Results</a></li>
        </ul>
    </nav>
</header>

<div id="main-wrap">
    <main>
        <section id="abstract">
            <h1>Abstract</h1>
            <p>
                <dfn>Morphemes</dfn> are the smallest units of meaning in a language. They play an important role in creating
                meaning and grammatical syntax. This is especially true in agglutinative languages, such as the Nguni languages,
                which construct words by concatenating many morphemes.Many Natural Language Processing (NLP) tasks, such as
                machine translation, can be improved by incorporating morphological information. However, methods to extract
                this information for the Nguni languages still need to be refined.

            <p>
                This paper evaluates the use of neural  methods in morphological parsing, or the labelling of morphemes
                with their corresponding grammatical role. We compare two main approaches:
                <ul>
                    <li><a href="#from-scratch">Training neural models from scratch</a>, and</li>
                    <li><a href="#plms">Fine-tuning pre-trained language models.</a></li>
                </ul>

            <p>We <a href="#results">compared the performance</a> of these models with
                each-other as well as with traditional methods of solving the task, such as Finite-State Transducer (FST) models.

            <p>
                We found that models trained from scratch outperformed both fine-tuned pre-trained language models and the
                traditional FST model. Models using morpheme-level embeddings and sentence-level context tended to perform the
                best.
        </section>

        <section id="introduction">
            <h1>Introduction</h1>

            <p>
                Morphemes are the basic building blocks of meaning (semantics) in a language. By understanding the
                grammatical role that morphemes play in a sentence, we can solve downstream Natural Language Processing
                (NLP) tasks better. These tasks range from information retrieval to machine translation.

            <!-- TODO we should explain morphological segmentation here -->

            <p>
                <dfn id="morph-parse-dfn">Morphological parsing</dfn> is the task of identifying the grammatical role
                of each morpheme within a word.  For example, "zobomi" (meaning "of life" in isiXhosa) is
                split into the morphemes "za-u-(bu)-bomi", which is parsed as "za[PossConc14] - u[NPrePre14] - (bu)[BPre14] - bomi[NStem]"
                The goal of morphological parsing is to predict these morpheme tags for arbitrary text. In our project
                we focus on the tagging step, and text that is already segmented by another algorithm.
            </p>

            <figure>
                <img src="files/parse-example.svg">

                <figcaption>Each bracketed tag labels the preceding morpheme with its grammatical function and noun class. The
                morpheme "bomi" is the word's <em>noun stem</em> in this example.
                </figcaption>
            </figure>

            <p>
                The <dfn>Nguni</dfn> languages are a group of widely-spoken South African languages, which include
                IsiXhosa, IsiZulu, IsiNdebele, and SiSwati. These languages are <dfn>low-resourced</dfn>, meaning that
                there are few tools and <dfn>corpora</dfn> (collections of documents) available for these languages.
            </p>

            <figure>
                <img src="files/nguni-speakers.svg" alt="A map of South Africa showing what proportion
                of people speak a Nguni language at home. Speakership is most dominant in the south-east of the country,
                covering almost the whole of the Eastern Cape and Kwa-Zulu Natal, but stretching into Gauteng and
                Mpumalanga.">
                <figcaption>Proportion of South Africans who speak a Nguni language at home.
                    <a href="https://en.wikipedia.org/wiki/File:South_Africa_2011_Nguni_speakers_proportion_map.svg">(Original
                    on Wikipedia</a>, public domain)
                </figcaption>
            </figure>

            <p>
                Morphological information is especially important for NLP in Nguni languages for two main reasons:
                <ul>
                    <li>
                        Nguni languages are <dfn>agglutinative</dfn>, meaning many words are created by combining
                        multiple morphemes.
                    </li>

                    <li>
                        Nguni languages are written conjunctively, meaning that morphemes are concatenated into a single
                        word. For example, in isiXhosa, "andikambuzi" means "I haven't yet asked him", and is
                        composed of the morphemes "a", "ndi", "ka", "m", "buza", and "i".
                    </li>
                </ul>

            <p>
                Few morphological parsers exist for the Nguni languages. One example of a morphological parser is the
                rule-based <a href="https://portal.sadilar.org/FiniteState/demo/zulmorph/">ZulMorph parser for isiZulu.</a>
                Rule-based parsers require linguists to manually incorporate stems, affixes, and grammar rules into the
                software. This is a tedious process which requires a high degree of expertise.

            <p>
                By comparison, machine-learning approaches are data driven. Instead of manually incorporating information
                into the algorithm, the parser can be automatically generated from linguistically-annotated data. This
                means that the process is language agnostic and can leverage previously-existing datasets.

            <p>
                In this project, we investigated the use of neural methods for morphological parsing of Nguni languages.
                We took two main approaches to this:
                <ul>
                    <li>
                        Simbarashe Mawere examined the fine-tuning of pre-trained language models. Three different
                        pre-trained language models (PLMs) were evaluated, with varying levels of inclusion of Nguni
                        Languages. These were XLM-Roberta (XLM), Afro-XLMR, and Nguni-XLMR.
                    </li>
                    <li>
                        Cael Marquard examined the use of training models from scratch. Two kinds of models were evaluated:
                        bidirectional Long Short-Term Memory (bi-LSTM) models, and neural Conditional Random Fields (CRFs).
                    </li>
                </ul>

            <p>
                The three research questions that we aimed to answer were:
                <ol>
                    <li><strong>Can neural approaches outperform traditional approaches to morphological parsing for the Nguni
                    languages?</strong></li>
                    <li><strong>Do models trained from scratch or fine-tuned pre-trained language models perform better?</strong></li>
                    <li><strong>Do models classifying surface segmentations or models classifying canonical segmentations
                    perform better?</strong></li>
                </ol>

            <p>
                In order to answer these questions, we compared the tagging quality of these models to each-other and
                to a traditional, rule-based approach
                (<a href="https://portal.sadilar.org/FiniteState/demo/zulmorph/">ZulMorph</a>) as a baseline. Comparisons
                were also made across segmentation type.
        </section>

        <!-- TODO separate aims maybe? -->
        <!-- TODO methodology -->

        <section id="from-scratch">
            <h1>Models Trained From Scratch</h1>
            <p class="author">Author: Cael Marquard</p>
            <p>Details about the exploration.</p>
            <div class="exploration-content">
                <div class="pdf-item">
                    <h3>Research Paper</h3>
                    <embed class="pdf-embed" src="files/MRQCAE001_MorphParse_report.pdf" type="application/pdf">
                </div>
                <div class="pdf-item">
                    <h3>Literature Review</h3>
                    <embed class="pdf-embed" src="files/MRQCAE001_MorphParse_literature_review.pdf" type="application/pdf">
                </div>
            </div>
        </section>

        <section id="plms">
            <h1>Pre-trained Language Models</h1>
            <p class="author">Author: Simbarashe Mawere</p>
            <p>Details about the exploration.</p>
            <div class="exploration-content">
                <div class="pdf-item">
                    <h3>Research Paper</h3>
                    <embed class="pdf-embed" src="files/MWRSIM003_MorphParse_Final_Report_site_version.pdf" type="application/pdf">
                </div>
                <div class="pdf-item">
                    <h3>Literature Review</h3>
                    <embed class="pdf-embed" src="path_to_your_literature_review2.pdf" type="application/pdf">
                </div>
            </div>
        </section>

        <section id="results">
            <h1>Results</h1>
            <p>
                Project results
            </p>
        </section>
    </main>
</div>

<footer>
    <p>&copy; 2024 MorphParse | By Simbarashe Mawere and Cael Marquard</p>
</footer>

<script src="script.js"></script>
</body>
</html>
