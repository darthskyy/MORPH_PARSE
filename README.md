# MORPH_PARSE

Morphological parsing NLP project by Cael Marquard and Simbarashe Mawere.

Copyright Â© [2024] [Cael Marquard & Simbarashe Mawere]

# From scratch models

Models trained from scratch were programmed and investigated by Cael.

## Setup & running

All instructions are for a Linux system, but with light modification would work on MacOS & Windows too.

**Requirements:**
- Python 3.10

**Steps:**

1. Create a virtualenv - `python3 -m venv venv/`
2. Activate venv - `source venv/bin/activate`
3. Install requirements.txt - `pip install -r requirements.txt`

At this point, you should be able to run any of the scripts. All scripts should be run from the main repository
directory, e.g. `python3 scripts/data_prep.py`, or `python3 from_scratch/fine_tune_bilstm_sentence.py`.

### Running the demo

To run the demo, simply run `python3 from_scratch/demo.py`. This will then prompt you for the model you want to run.
These can be found in `out_models`. Once selected, type the folder and filename, e.g.
`bilstm_sentence_morpheme_canon/bilstm-sentences-morpheme-NR.pt`

### Training models

To train models, the code found in `fine_tune_bilstm_sentence.py` or `fine_tune_crf_morpheme_sentence.py` can be adjusted.
It is recommended to simply copy these files, rename them, and change the parameters. By themselves, they serve as fully
functional templates for training & hyperparameter tuning. When running these files, if you wish to save the models,
pass a directory through the `MODEL_OUT_DIR` environment variable. During tuning, pass a directory for intermediate
results & checkpoints through the `TUNING_CHECKPOINT_DIR` environment variable.

To tune the models to find the best hyperparameters, replace the invocation of `final_train()` with `fine_tune()` (and
vice versa for training).

The split (context level) can be adjusted (options found in `common.py`), as can the feature level, simply by adjusting
the `splits` and `feature_level` variables.

Valid splits include:
- `split_sentences`
- `split_words`

Valid feature levels include (last two items of tuple):
- `tokenize_into_morphemes, lambda config, dset: EmbedSingletonFeature(dset, config["embed_target_embed"])`
- `tokenize_into_chars, lambda config, dset: EmbedBySumming(dset, config["embed_target_embed"])`

The `cfg` variable in the `fine_tune` function can be adjusted to use any kinds of `ray.tune` tuning function
(e.g. `tune.loguniform`). Once the best hyperparameters are selected, the `cfg` variable in the `final_train` function
can be adjusted.

In order to customise the training and tuning process, different arguments can be passed to `train_all` and `tune_model`.

For `tune_model`, these include:
- `cpus`: number of CPUs to use whilst tuning
- `hrs`: how many hours to tune for
- `epochs`: how many epochs to run at maximum

For `train_all`, these include:
- `langs`: which languages to train for (default is all)
- `use_surface`: whether to train/eval on surface segmentations 
- `n_models`: how many seeds to train (max 5)

To tune on surface segmentations, make sure to pass `use_surface=True` as a keyword argument.

## Folder structure

- from_scratch/ : code and scripts for models trained from scratch
  - `aligned_f1.py`: maximal alignment algorithm
  - `bilstm_crf.py`: code for the Bi-LSTM CRF classifier
  - `lstm.py`: code for the Bi-LSTM classifier
  - `common.py`: training, tuning, and other functions shared between all models
  - `dataset.py`: data loading and preparation
  - `demo.py`: model loading for demonstration
  - `encapsulated_model.py`: save models alongside required dictionaries so that they can be loaded independent of the
    dataset
  - `fine_tune_bilstm_sentence.py`: example code for training or tuning a Bi-LSTM sentence-level classifier
  - `fine_tune_crf_morpheme_sentence.py`: example code for training or tuning a Bi-LSTM CRF sentence-level classifier
- out_models/ : all saved models (folder & file names indicate types and languages)
- scripts/
  - `aligned_f1.py`: maximal alignment algorithm
  - `data_prep.py`: prepare {lang}_{TRAIN|TEST}.tsv files
  - `eval_segmenter.py`: perform maximally-aligned F1 score evaluation of segmentation accuracy
  - `generate_gold_surface_segs.py`: generates gold-standard surface segmentations
  - `parse_train_out.py`: utility script which finds best validset epoch for a model and takes the corresponding one
    from the full-trainset run
  - `prep_test_surface_segs.py`: prepare {lang}_TEST_SURFACE.tsv files
  - `scrape_zulmorph.py`: scrapes raw ZulMorph output for the dataset's testset
  - `zulmorph_analysis.py`: analyses ZulMorph's segmentation & tagging quality
- data/
  - Surface/ : dataset adapted to surface segmentations
    - gold/ : "gold-standard" surface segmentations as generated by `scripts/generate_gold_surface_segs.py`
    - predicted/ : predicted surface segmentation as generated by
      [Moeng et al.'s feature-based CRFs][segment]
  - TEST/ : test portion of dataset (raw & prepared forms)
    - {lang}_TEST.tsv: test file with gold-standard canonical segmentations
    - {lang}_TEST_CANONICAL_PRED.tsv: test file with predicted canonical segmentations (generated by
      [Moeng et al.'s canonical transformers][segment])
    - {lang}_TEST_SURFACE.tsv: test file with predicted surface segmentations (generated by
      [Moeng et al.'s feature-based CRFs][segment])
    - {lang}_TESTSET_GOLD_SURFACE.tsv: test file with gold-standard surface segmentations (generated by
     `scripts/generte_gold_surface_segs.py`)
    - SADII.*: raw testset files
  - TRAIN/ : train portion of dataset
    - {lang}_TRAIN.tsv: train file with gold-standard canonical segmentations
    - {lang}_TRAIN_SURFACE.tsv: train file with gold-standard surface segmentations
    - SADII.*: raw trainset files
  - ZulMorph/ : ZulMorph-scraped analyses and segmentations of the dataset

## Citations & things used
- [Linguistically enriched corpora for conjunctively written South African languages](https://repo.sadilar.org/handle/20.500.12185/546?show=full)
  (licensed CC BY 4.0)
- `scripts/generate_gold_surface_segs` adapted from code by Jan Buys (used with permission)
- [pos-nguni](https://github.com/francois-meyer/pos-nguni/) by Francois Meyer (used with permission)
- Pretorius, L. and Bosch, S. (2018). ZulMorph: Finite state morphological analyser for Zulu (Version 20190103) [Software]. Web demo at https://portal.sadilar.org/FiniteState/demo/zulmorph/ 
- [MORPH_SEGMENT](https://github.com/DarkPr0digy/MORPH_SEGMENT) by Moeng et al.  

[segment]: https://projects.cs.uct.ac.za/honsproj/cgi-bin/view/2020/daniels_moeng_reay.zip/MORPH_SEGMENT/MORPH-SEGMENT.html